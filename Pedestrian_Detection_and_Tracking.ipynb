{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Overview"
      ],
      "metadata": {
        "id": "DjWRjH29eMtI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Object Detection"
      ],
      "metadata": {
        "id": "2-qWJe_AeQgW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Object detection** involves identifying and precisely locating particular objects in images or videos. It encompasses the task of detecting and pinpointing multiple objects within a scene.\n",
        "\n",
        "In computer vision and image processing, object detection algorithms analyze visual data to automatically recognize objects by drawing bounding boxes around them. This essential technology is extensively applied in various domains such as autonomous driving, surveillance systems, robotics, augmented reality, and many others."
      ],
      "metadata": {
        "id": "Vy9u636Iewly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Object Tracking"
      ],
      "metadata": {
        "id": "v_TjiIKxeThb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Object tracking** is the ongoing process of monitoring the movement of one or more objects throughout a video by accurately determining their position, size, and other relevant features.\n",
        "\n",
        "It is a crucial task in computer vision and video analysis with applications in surveillance systems, autonomous vehicles, robotics, human-computer interaction, and other fields."
      ],
      "metadata": {
        "id": "uQxzGhs6fOHY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pedestrian Detection and Tracking Task"
      ],
      "metadata": {
        "id": "ifLDkpa8fyIz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Goals"
      ],
      "metadata": {
        "id": "j9enEeBOzq8u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project aims to create a system capable of identifying and monitoring people in video. The main objective is to accurately detect and track the presence, position, and movement of people within a given scene.\n",
        "\n",
        "Once pedestrian are detected, the system will focus on continuously tracking their movements across frames. The tracking algorithm will ensure reliable and precise tracking, maintaining the identity and trajectory of each person throughout the video sequence.\n",
        "\n",
        "The project's outcomes will have practical applications in various domains. For example, it can be used in traffic surveillance systems to monitor congestion, detect suspicious activities, or analyze traffic patterns. It can also contribute to advancements in autonomous driving technologies by providing real-time information about the positions and movements of surrounding vehicles.\n",
        "\n",
        "To achieve its goals, the project will employ a combination of image processing, object detection, and tracking algorithms, utilizing both traditional computer vision methods and modern deep learning approaches. The ultimate aim is to develop an efficient and dependable car detection and tracking system capable of real-time operation, providing accurate results for improved situational awareness and decision-making."
      ],
      "metadata": {
        "id": "IXYfRr1af4CT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Project Outlines"
      ],
      "metadata": {
        "id": "Y3gxzkLwzswH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MobileNet SSD (Single Shot MultiBox Detector)"
      ],
      "metadata": {
        "id": "d3ANGm5Kz0ai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MobileNet SSD model, which is a variant of the SSD architecture, is employed in this project. It takes advantage of the MobileNet convolutional neural network as its core, merging the MobileNet model's efficiency with the object detection capabilities of SSD. This combination enables real-time object detection on devices with limited resources, such as mobile phones and embedded systems."
      ],
      "metadata": {
        "id": "Qij6sLcFz-ZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Object tracking"
      ],
      "metadata": {
        "id": "SSHJ2STb0S5U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Algorithm's Descriptions (Centroid Tracking approach).**\n",
        " - *General idea:*\n",
        "  - The idea of how to keep track of pedestrians across frames/over time is to utilize the Euclidean distance to track the existing object centroid from the previous frames and new object centroids detected in the current processing frame.\n",
        "  - Using the pre-trained MobileNet SSD, we are able to detect pedestrians existing in the current processing frames and we are also able to draw bounding boxes around the object. Centroids calculated from the bounding are essential in this tracking algorithm because it is fairly to assume that any pedestrian's position in the next subsequent frame cannot be too far away from his position in the previous frame.\n",
        " - *Assumptions:*\n",
        "  - The pedestrian's position in the next subsequent frame cannot be too far away from his position in the previous frame (a.k.a, the person doesn't move too fast)\n",
        "  - The distance between the centroids of the same pedestrians in the current and next frame is smaller than all other distances between pedestrians\n",
        " - *STEPS (Each step is performed frame by frame)*\n",
        "  - <u>Step 1:</u> Detect the pedestrians and calculate the objects' centroids\n",
        "  - <u>Step 2:</u> Compute the Euclidean distance of each pair between the new objects' centroids and the previously tracked centroid.\n",
        "  - <u>Step 3:</u> Update information for old and new objects by matching the current input centroids to the previously stored centroids, assuming that the distance between the centroids of the same object for two subsequent frames will be smaller than all other distances between other objects.\n",
        "    - If we are able to find matching objects, update the centroids' information of matching objects.\n",
        "    - If there are more detected centroids in this frame compared to the existing tracked centroids, we will need to register the new objects\n",
        "    - If there are fewer detected centroids in this frame compared to the existing tracked centroids (a.k.a, some objects disappeared in this current frame), we need to check for those objects' number of disappearances. If their number of disappearances has exceeded the specified maximum times (in this project, we set 50 as the maximum number of disappearances of an object), it means that we will deregister these objects.\n",
        "\n",
        "**2. Limitations of the Algorithm**\n",
        " - In order for this algorithm to work, we need to assume that centroids must lie close together between subsequent frames.\n",
        " - If an object overlaps with another one, it can lead to object ID switching\n",
        " - It could be computationally expensive if we are required to track multiple classes (in this assignment, it may not be a problem since we are only required to track and detect pedestrians).\n"
      ],
      "metadata": {
        "id": "lvJZGVf60xeq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Detect up to 3 closest pedestrians to the camera"
      ],
      "metadata": {
        "id": "a5p5C0BS2fN8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Algorithm's Descriptions (Triangle similarity).**\n",
        " - *General idea:*\n",
        "  - This is based on triangle similarity. Let's assume that we know the width of a pedestrian in reality (W) and this person stands D distance from the camera. If I take a picture of this person using the camera, we can then know the apparent width of that person in pixels (P) in the image. This allows us to calculate the estimated focal length F of our camera using the formula:\n",
        "\n",
        "  \\begin{align}\n",
        "    F = \\frac{P * D}{W} \\tag{1}\n",
        "  \\end{align}\n",
        "\n",
        "          where     F: focal length\n",
        "                    P: the apparent width in pixels\n",
        "                    D: distance of the object from our camera\n",
        "                    W: known width\n",
        "  - Based on information about the focal length, the apparent width in pixels, and the known width of the pedestrians, we are able to apply the triangle similarity to calculate the distance of the object to the camera as the object moves closer to or farther away from the camera using the formula:\n",
        "  \\begin{align}\n",
        "    D' = \\frac{W * F}{P}   \\tag{2}\n",
        "  \\end{align}\n",
        "          where   D': Estimated distance between the object and the camera\n",
        "                  W: known width\n",
        "                  F: focal length\n",
        "                  P: the apparent width in pixels of the detected object\n",
        " - *STEPS:*\n",
        "  - <u>Step 1:</u>\n",
        "    - Determine a pedestrian's width (W in formula (1)):\n",
        "      - I will assume the width of a pedestrian in centimeters. If drawing a box around a person, I can assume that the width of the box equals to that person's shoulder width. According to https://www.healthline.com/health/average-shoulder-width website, the average shoulder width of American women is 36.7 cm and 41.1 cm are the averages for American men's shoulder width. Therefore, let 38.9 which is the average of the average American women. and men's shoulder width be the assumed width of a pedestrian in the real world.\n",
        "    - Determine the known distance from the camera to the object (D in formula (1)):\n",
        "      - Since the input video varies, I don't have any reference to the distance between the camera and the object. Let's assume that the distance between the object and the camera is 200 centimeters.\n",
        "    - Find the apparent width in pixel (P in formula (1)):\n",
        "     - This is a challenging task because as I mention before, we have dynamic inputs and we don't have any reference for each video context. However, I came up with an idea to find the apparent width of a pedestrian in pixels. For any input videos, I use the pre-trained MobileNet SSD to detect pedestrians. For the first frame that any pedestrians are detected, I will calculate the width of the bounding boxes of all detected pedestrians and store them in a list. I will take the median of all pedestrians' widths in the frame that pedestrians are first detected as the estimated apparent width of a pedestrian in that input video. The reason why I choose to use the median as the representation is that the median is better at capturing the typical width and it is more robust to outliers. Also, the reason why I only try to take the frame where pedestrians are first detected as the reference is because of the computational cost.\n",
        "     - Important Note: The apparent width calculated in this step will only be used ONCE to estimate the focal length at the beginning. After the focal length is calculated, we will use the apparent width of each detected object in each frame to calculate the estimated distance between each object and the camera.\n",
        "  - <u>Step 2:</u> Calculate the focal length using the formula: F = (P * D) / W\n",
        "  - <u>Step 3:</u> Calculate the estimated distance using the idea of triangle similarity.\n",
        "   - Frame by frame, we will calculate the estimated distance between each object to the camera using the formula: D' = (W * F) / P\n",
        "    - As mentioned in step 1 above, the apparent width here is different from the apparent width used to estimate the focal length. The apparent width used in this step is the unique apparent width of each object in a single frame.\n",
        "    - After the estimated distance is calculated, it will be stored if the distance is in the top 3 of the shortest distance between detected pedestrians and the camera in that frame.\n",
        "\n",
        "\n",
        "**2. Limitations of the Algorithm**\n",
        "  - One limitation of this method is that it works best if we have a straight-on view of the object we are detecting. As the viewpoint becomes angled, it can lead to the distorted calculation of the apparent widths.\n",
        "  - Also, as mentioned above, we all estimate the real width of a pedestrian, the distance between a pedestrian and the camera, and the apparent width of a pedestrian in pixels to calculate the focal length; therefore, the estimated distance between detected pedestrians and camera is not the correct distance in reality."
      ],
      "metadata": {
        "id": "L5tcTED523SJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Codes"
      ],
      "metadata": {
        "id": "iP7qXRHJxjNc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9Ym9wyLynKd"
      },
      "outputs": [],
      "source": [
        "import cv2 as cv\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "import argparse"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_pretrained_model():\n",
        "    \"\"\"\n",
        "    Read the pretrained MobileNet SSD (Single Shot Detector) model\n",
        "\n",
        "        None\n",
        "\n",
        "    Return:\n",
        "        model: the loaded model\n",
        "        class_names: 90 possible detected classes\n",
        "    \"\"\"\n",
        "    # load the COCO class names\n",
        "    with open('MobileNet SSD-COCO Model/object_detection_classes_coco.txt', 'r') as f:\n",
        "        class_names = f.read().split('\\n')\n",
        "\n",
        "    # load the DNN model\n",
        "    model = cv.dnn.readNet(model='MobileNet SSD-COCO Model/frozen_inference_graph.pb',\n",
        "                            config='MobileNet SSD-COCO Model/ssd_mobilenet_v2_coco_2018_03_29.pbtxt.txt',\n",
        "                            framework='TensorFlow')\n",
        "\n",
        "    return model, class_names"
      ],
      "metadata": {
        "id": "ybdFvtm9zP9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CentroidTracker():\n",
        "    \"\"\"\n",
        "    A class helps to keep track of objects' apperance and assign unique ID to each object\n",
        "\n",
        "    REFERENCE:  The CentroidTracker() class was written based on Adrian Rosebrock's blog published on July 23, 2018 and was modified for the purpose of this task.\n",
        "                Link to Adrian Rosebrock's blog: https://pyimagesearch.com/2018/07/23/simple-object-tracking-with-opencv/\n",
        "    \"\"\"\n",
        "    def __init__(self, maxDisappeared=50):\n",
        "        # initialize the next unique object ID along with two ordered dictionaries used to keep track of mapping a given object\n",
        "        # ID to its centroid and number of consecutive frames it has been marked as \"disappeared\", respectively\n",
        "        self.nextObjectID = 1\n",
        "        self.objects = OrderedDict()\n",
        "        self.disappeared = OrderedDict()\n",
        "\n",
        "        # store the number of maximum consecutive frames a given object is allowed to be marked as \"disappeared\" until we\n",
        "        # need to deregister the object from tracking\n",
        "        self.maxDisappeared = maxDisappeared\n",
        "\n",
        "\n",
        "    def calculate_euclidean_dist(self, objectCentroids, inputCentroids):\n",
        "        \"\"\"\n",
        "        Calculate the Euclidean distance between each pair of existing centroids in the previous frames and the detected centroids in\n",
        "        the current frame.\n",
        "\n",
        "            objectCentroids: a list of existing centroids in the preivous frames\n",
        "            inputCentroids: an array of detected centroids in the current frames\n",
        "\n",
        "        Return:\n",
        "            D: an array of Euclidean distance between all possible pairs of existing and inputing centroids\n",
        "        \"\"\"\n",
        "        D = []\n",
        "\n",
        "        for obj_centroid in np.array(objectCentroids):\n",
        "            dist_arr = []\n",
        "            for input_centroid in inputCentroids:\n",
        "                dist = np.linalg.norm(obj_centroid - input_centroid)\n",
        "                dist_arr.append(dist)\n",
        "            D.append(dist_arr)\n",
        "\n",
        "        D = np.asarray(D)\n",
        "\n",
        "        return D\n",
        "\n",
        "\n",
        "    def register(self, centroid):\n",
        "        \"\"\"\n",
        "        Register a ID for a new detected object\n",
        "\n",
        "            centroid: a centroid of a new detected object\n",
        "\n",
        "        Return:\n",
        "            None\n",
        "        \"\"\"\n",
        "        # when registering an object we use the next available object ID to store the centroid\n",
        "        self.objects[self.nextObjectID] = centroid\n",
        "        self.disappeared[self.nextObjectID] = 0\n",
        "        self.nextObjectID += 1\n",
        "\n",
        "\n",
        "    def deregister(self, objectID):\n",
        "        \"\"\"\n",
        "        Deregister for an object that disappears for more than the maximum number allowed an object to disappear\n",
        "\n",
        "            objectID: an ID of an object\n",
        "\n",
        "        Return:\n",
        "            None\n",
        "        \"\"\"\n",
        "        # to deregister an object ID we delete the object ID from both of our respective dictionaries\n",
        "        del self.objects[objectID]\n",
        "        del self.disappeared[objectID]\n",
        "\n",
        "\n",
        "    def update(self, rects):\n",
        "        \"\"\"\n",
        "        Update the tracking information over time (register new objects/deregister disappered objects/update centroids of current objects)\n",
        "\n",
        "            rects: Information of the bounding box of the detected object\n",
        "\n",
        "        Return:\n",
        "            self.objects: A dictionary with keys as IDs and values as objects' centroids\n",
        "        \"\"\"\n",
        "        # check to see if the list of input bounding box rectangles is empty\n",
        "        if len(rects) == 0:\n",
        "            # loop over any existing tracked objects and mark them as disappeared\n",
        "            for objectID in self.disappeared.keys():\n",
        "                self.disappeared[objectID] += 1\n",
        "\n",
        "                # if we have reached a maximum number of consecutive frames where a given object has been marked as missing, deregister it\n",
        "                if self.disappeared[objectID] > self.maxDisappeared:\n",
        "                    self.deregister(objectID)\n",
        "\n",
        "            # return early as there are no centroids or tracking info to update\n",
        "            return self.objects\n",
        "\n",
        "        # initialize an array of input centroids for the current frame\n",
        "        inputCentroids = np.zeros((len(rects), 2), dtype=\"int\")\n",
        "\n",
        "        # loop over the bounding box rectangles\n",
        "        for (i, (startX, startY, endX, endY)) in enumerate(rects):\n",
        "            # use the bounding box coordinates to derive the centroid\n",
        "            cX = int((startX + endX) / 2.0)\n",
        "            cY = int((startY + endY) / 2.0)\n",
        "            inputCentroids[i] = (cX, cY)\n",
        "\n",
        "        # if we are currently not tracking any objects take the input centroids and register each of them\n",
        "        if len(self.objects) == 0:\n",
        "            for i in range(0, len(inputCentroids)):\n",
        "                self.register(inputCentroids[i])\n",
        "\n",
        "        # otherwise, are are currently tracking objects so we need to try to match the input centroids to existing object centroids\n",
        "        else:\n",
        "            # grab the set of object IDs and corresponding centroids\n",
        "            objectIDs = list(self.objects.keys())\n",
        "            objectCentroids = list(self.objects.values())\n",
        "\n",
        "            # compute the distance between each pair of object centroids and input centroids, respectively -- our\n",
        "            # goal will be to match an input centroid to an existing object centroid\n",
        "            D = self.calculate_euclidean_dist(np.array(objectCentroids), inputCentroids)\n",
        "\n",
        "            # in order to perform this matching we must (1) find the smallest value in each row and then (2) sort the row\n",
        "            # indexes based on their minimum values so that the row with the smallest value as at the *front* of the index list\n",
        "            rows = D.min(axis=1).argsort()\n",
        "\n",
        "            # next, we perform a similar process on the columns by finding the smallest value in each column and then\n",
        "            # sorting using the previously computed row index list\n",
        "            cols = D.argmin(axis=1)[rows]\n",
        "\n",
        "            # in order to determine if we need to update, register, or deregister an object we need to keep track of which\n",
        "            # of the rows and column indexes we have already examined\n",
        "            usedRows = set()\n",
        "            usedCols = set()\n",
        "\n",
        "            # loop over the combination of the (row, column) index tuples\n",
        "            for (row, col) in zip(rows, cols):\n",
        "                # if we have already examined either the row or column value before, ignore it val\n",
        "                if row in usedRows or col in usedCols:\n",
        "                    continue\n",
        "\n",
        "                # otherwise, grab the object ID for the current row, set its new centroid, and reset the disappeared counter\n",
        "                objectID = objectIDs[row]\n",
        "                self.objects[objectID] = inputCentroids[col]\n",
        "                self.disappeared[objectID] = 0\n",
        "\n",
        "                # indicate that we have examined each of the row and column indexes, respectively\n",
        "                usedRows.add(row)\n",
        "                usedCols.add(col)\n",
        "\n",
        "            # compute both the row and column index we have NOT yet examined\n",
        "            unusedRows = set(range(0, D.shape[0])).difference(usedRows)\n",
        "            unusedCols = set(range(0, D.shape[1])).difference(usedCols)\n",
        "\n",
        "            # in the event that the number of object centroids is equal or greater than the number of input centroids\n",
        "            # we need to check and see if some of these objects have potentially disappeared\n",
        "            if D.shape[0] >= D.shape[1]:\n",
        "                # loop over the unused row indexes\n",
        "                for row in unusedRows:\n",
        "                    # grab the object ID for the corresponding row index and increment the disappeared counter\n",
        "                    objectID = objectIDs[row]\n",
        "                    self.disappeared[objectID] += 1\n",
        "\n",
        "                    # check to see if the number of consecutive frames the object has been marked \"disappeared\" for warrants deregistering the object\n",
        "                    if self.disappeared[objectID] > self.maxDisappeared:\n",
        "                        self.deregister(objectID)\n",
        "\n",
        "            # otherwise, if the number of input centroids is greater than the number of existing object centroids we need to\n",
        "            # register each new input centroid as a trackable object\n",
        "            else:\n",
        "                for col in unusedCols:\n",
        "                    self.register(inputCentroids[col])\n",
        "\n",
        "        # return the set of trackable objects\n",
        "        return self.objects"
      ],
      "metadata": {
        "id": "nw4o5z0AzW3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_focal_length(est_dist_btw_obj_cam, real_object_width, object_width_in_image):\n",
        "    \"\"\"\n",
        "    Estimate the focal length\n",
        "\n",
        "        est_dist_btw_obj_cam: an estimated distance between the object and camera\n",
        "        real_object_width: a measured width of detected object in reality\n",
        "        object_width_in_image: a width of the object displayed in the image (pixels)\n",
        "\n",
        "    Return:\n",
        "        focal_length: The estimated focal length\n",
        "    \"\"\"\n",
        "    focal_length = (object_width_in_image * est_dist_btw_obj_cam) / real_object_width\n",
        "\n",
        "    return focal_length\n",
        "\n",
        "def estimate_distance(focal_length, real_object_width, object_width_in_image):\n",
        "    \"\"\"\n",
        "    Estimate distance between a detected object and the camera\n",
        "\n",
        "        focal_length: an estimated focal length\n",
        "        real_object_width: a measured width of detected object in reality\n",
        "        object_width_in_image: a width of the object displayed in the image (pixels)\n",
        "\n",
        "    Return:\n",
        "        distance: the estimated distance between a detected object and the camera\n",
        "    \"\"\"\n",
        "    distance = (real_object_width * focal_length) / object_width_in_image\n",
        "\n",
        "    return distance"
      ],
      "metadata": {
        "id": "Q1UdnB51zaU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_obj_width_in_pic(cap, model, class_names):\n",
        "    \"\"\"\n",
        "    Estimate the width of an object in a single frame (pixels).\n",
        "\n",
        "        cap: a video file sequence\n",
        "        model: a loaded model\n",
        "        class_names: class names that the loaded model can detect\n",
        "\n",
        "    Return:\n",
        "        The estimated width of an object in a single frame (pixels)\n",
        "    \"\"\"\n",
        "    first_time_detected_obj = True\n",
        "    object_width_first_frame = [] # Widths of all targeted objects detected in the first frame\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        image = frame.copy()\n",
        "        _, image_width, _ = image.shape\n",
        "\n",
        "        # create blob from image\n",
        "        blob = cv.dnn.blobFromImage(image=image, size=(300, 300), mean=(104, 117, 123), swapRB=True)\n",
        "        model.setInput(blob)\n",
        "        output = model.forward()\n",
        "\n",
        "        # loop over each of the detections\n",
        "        for detection in output[0, 0, :, :]:\n",
        "            # Extract the confidence of the detection\n",
        "            confidence = detection[2]\n",
        "\n",
        "            # Get the class_id\n",
        "            class_id= detection[1]\n",
        "            # Map the class id to the class\n",
        "            class_name = class_names[int(class_id)-1]\n",
        "\n",
        "            # draw bounding boxes only if the detection confidence os above a certain threshold, else skip\n",
        "            if class_name == \"person\" and confidence > 0.4:\n",
        "                # Get the boudning box coordinates\n",
        "                x_start = detection[3] * image_width\n",
        "\n",
        "                # Get the boudning box width\n",
        "                x_end = detection[5] * image_width\n",
        "                w = x_end - x_start\n",
        "                object_width_first_frame.append(w)\n",
        "                first_time_detected_obj = False # change to False if we found the first frame that contains the target object\n",
        "\n",
        "        # Break if we have already found the first frame that contains target objectss\n",
        "        if not first_time_detected_obj:\n",
        "            break\n",
        "\n",
        "    object_width_first_frame = np.asarray(object_width_first_frame)\n",
        "\n",
        "    # Return the median of all detected width\n",
        "    return np.median(object_width_first_frame)"
      ],
      "metadata": {
        "id": "NbFIkRDFzfYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def task_1(file_name):\n",
        "    \"\"\"\n",
        "    Perform task 1 including extracting moving objects using Gaussian Mixture background modelling, removing noisy detection using morphological operators or\n",
        "    majority voting, counting separate moving objects using connected component analysis, and classifying each object (or connected component) into person,\n",
        "    car and other by simply using the ratio of width and height of the connected components\n",
        "\n",
        "        file_name: input video file name\n",
        "\n",
        "    Return:\n",
        "        Output video frame\n",
        "        The number of objects or connected components\n",
        "    \"\"\"\n",
        "    # Read video\n",
        "    cap = cv.VideoCapture(file_name)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Unable to open {file_name}\")\n",
        "\n",
        "    # Creates MOG2 Background Subtractor (Gaussian Mixture-based Background/Foreground Segmentation)\n",
        "    back_ground_sub = cv.createBackgroundSubtractorMOG2()\n",
        "\n",
        "    frame_count = 0 # Number of frames\n",
        "    KERNEL = np.ones((5,5), np.uint8) # Define a kernel to perform morphological transformations\n",
        "    CONNECTIVITY = 8 # Connectivity used in the connected component analysis\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Resize the video frame to a size comparable to VGA\n",
        "        frame_resized = resize_image(frame)\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "        # Extract foreground (moving pixels)\n",
        "        foreground_mask = back_ground_sub.apply(frame_resized)\n",
        "        # Extract background\n",
        "        background_mask = back_ground_sub.getBackgroundImage()\n",
        "\n",
        "        # Remove noisy detection using morphological operators, in particular the opening operator\n",
        "        foreground_mask_noise_rm = cv.morphologyEx(foreground_mask, cv.MORPH_OPEN, KERNEL, iterations=1)\n",
        "        # Remove noise: if a pixel is less than 250, set it equals to 0\n",
        "        foreground_mask_noise_rm[np.abs(foreground_mask_noise_rm) < 250] = 0\n",
        "\n",
        "        # Perform connected component analysis\n",
        "        output = cv.connectedComponentsWithStats(foreground_mask_noise_rm, CONNECTIVITY, cv.CV_32S)\n",
        "        numLabels, labels, stats, _ = output\n",
        "\n",
        "        componentMask = np.uint8(labels)*255\n",
        "        detected_objects = cv.bitwise_and(frame_resized, frame_resized, mask=componentMask)\n",
        "\n",
        "        # Classify each object (or connected component) into person, car, and others by simply using the ratio of width and height and area of the connected component\n",
        "        # RULES TO CLASSIFY OBJECTS (PERSON, CAR, AND OTHERS:\n",
        "        #       - CAR: if the ratio between width and height is between 1.30 and 2.30 and the area of the bounding box is greater than or equal to 800\n",
        "        #       - PERSON: if the ratio between width and height is between 0.60 and 0.90 and the area of the bounding box is greater than or equal to 100\n",
        "        #       - OTHER: if the ratio does not fall into the above specified range and the area of the boudning box is greater than or equal to 50\n",
        "        if numLabels == 1:\n",
        "            # if there is only one label, it means that there is no object/connected component detected in this frame (only the background)\n",
        "            print(f\"Frame {frame_count:04d}: 0 object\")\n",
        "        else:\n",
        "            total_objects = 0 # Total number of detected objects\n",
        "            car_counts = 0 # Total number of detected cars\n",
        "            person_counts = 0 # Total number of detected persons\n",
        "            others_counts = 0 # Total number of detected other objects\n",
        "\n",
        "            for i in range (1, numLabels):\n",
        "                # Get the width, height, and area of the component\n",
        "                w = stats[i, cv.CC_STAT_WIDTH]\n",
        "                h = stats[i, cv.CC_STAT_HEIGHT]\n",
        "                area = stats[i, cv.CC_STAT_AREA]\n",
        "\n",
        "                # Calcualte the ratio between width and height of the component\n",
        "                width_height_ratio = round(w/h, 2)\n",
        "\n",
        "                # Classify objects\n",
        "                if width_height_ratio >= 1.30 and width_height_ratio <= 2.3 and area >= 800:\n",
        "                    # If it is classified as a car\n",
        "                    car_counts += 1\n",
        "                    total_objects += 1\n",
        "                elif width_height_ratio >= 0.60 and width_height_ratio <= 0.9 and area >= 100:\n",
        "                    # If it is classified as a person\n",
        "                    person_counts += 1\n",
        "                    total_objects += 1\n",
        "                else:\n",
        "                    if area >= 50:\n",
        "                        # If it is classified as others\n",
        "                        others_counts += 1\n",
        "                        total_objects += 1\n",
        "\n",
        "            # Text processing to display the output in the command window\n",
        "            total_object_text_display = \"object\"\n",
        "            car_text_display = \"car\"\n",
        "            person_text_display = \"person\"\n",
        "            other_text_display = \"other\"\n",
        "\n",
        "            if total_objects > 1:\n",
        "                total_object_text_display += \"s\"\n",
        "                car_text_display += \"s\" if car_counts > 1 else \"\"\n",
        "                person_text_display += \"s\" if person_counts > 1 else \"\"\n",
        "                other_text_display += \"s\" if others_counts > 1 else \"\"\n",
        "\n",
        "            print(f\"Frame {frame_count:04d}: {total_objects} {total_object_text_display} ({person_counts} {person_text_display}, {car_counts} {car_text_display}, {others_counts} {other_text_display})\")\n",
        "\n",
        "        # Concatenate to display\n",
        "        first_row = cv.hconcat([frame_resized, background_mask]) # LEFT: the original video frame, RIGHT: the estimated background frame\n",
        "        foreground_mask = cv.cvtColor(foreground_mask, cv.COLOR_GRAY2BGR)\n",
        "        second_row = cv.hconcat([foreground_mask, detected_objects]) # LEFT: The detected moving pixels before filtering (in binary mask), RIGHT: detected objects (in the orihinal color)\n",
        "        stack = cv.vconcat((first_row, second_row))\n",
        "\n",
        "        cv.imshow(\"Task 1's Output\", stack)\n",
        "\n",
        "        key = cv.waitKey(20)\n",
        "        if key == 27:\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    cv.destroyAllWindows()"
      ],
      "metadata": {
        "id": "5FXMGednzl4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def person_tracking(file_name):\n",
        "    \"\"\"\n",
        "    Perform task 2 including detecting pedestrians (i.e. persons) using a OpenCV Deep Neural Network (DNN) module and a MobileNet SSD detector pre-trained on\n",
        "    the MS COCO dataset, tracking and display the detected pedestrians by providing same labels to the same pedestrians across over times, and selecting up to\n",
        "    (3) pedestrians that are most close in space to the camera.\n",
        "\n",
        "        file_name: input video file name\n",
        "\n",
        "    Return:\n",
        "        Output video frame\n",
        "    \"\"\"\n",
        "    # Read video\n",
        "    cap = cv.VideoCapture(file_name)\n",
        "    cap_ = cv.VideoCapture(file_name)\n",
        "    model, CLASS_NAMES = read_pretrained_model()\n",
        "\n",
        "    # Initialize our centroid tracker and frame dimensions\n",
        "    ct = CentroidTracker()\n",
        "\n",
        "    # Assumption distance from camera to a pedestrian measured (in centimeter)\n",
        "    KNOWN_DISTANCE = 200\n",
        "    # Assumption width of a pedestrian in the real world (in centimeter). If drawing a box around a person, we can assume that the width of the box equals to the\n",
        "    # that person's shoulder width. According to https://www.healthline.com/health/average-shoulder-width website, the average shoulder width of American women is 36.7 cm\n",
        "    # and 41.1 cm is the average for American men's shoulder width. Therefore, let 38.9 which is the average of the average American women and men's shoulder width be the\n",
        "    # assumption width of a pedestrian in reall world.\n",
        "    KNOWN_WIDTH = 38.9\n",
        "\n",
        "    # Estimate a pedestrian width in pixels\n",
        "    ref_obj_width = estimate_obj_width_in_pic(cap_, model, CLASS_NAMES)\n",
        "    # Estimate focal length\n",
        "    focal_length_found = estimate_focal_length(KNOWN_DISTANCE, KNOWN_WIDTH, ref_obj_width)\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        image = frame.copy()\n",
        "        frame_1 = frame.copy()\n",
        "        frame_2 = frame.copy()\n",
        "        frame_3 = frame.copy()\n",
        "        frame_4 = frame.copy()\n",
        "\n",
        "        image_height, image_width, _ = image.shape\n",
        "        # create blob from image\n",
        "        blob = cv.dnn.blobFromImage(image=image, size=(300, 300), mean=(104, 117, 123), swapRB=True)\n",
        "        model.setInput(blob)\n",
        "        output = model.forward()\n",
        "\n",
        "        # Intialize necessary variables\n",
        "        rects = [] # Store location of the bounding boxes of detected pedestrians in each frame\n",
        "        centroids = [] # Store centroids' location of the bounding boxes of detected pedestrians in each frame\n",
        "        camera_obj_dist_tracking = [] # Store up to 3 closest distance between pedestrians and the camera\n",
        "        box_info = [] # Store the locations of the bounding of up to 3 closest pedestrians to the camera\n",
        "\n",
        "        # loop over each of the detections\n",
        "        for detection in output[0, 0, :, :]:\n",
        "            # Extract the confidence of the detection\n",
        "            confidence = detection[2]\n",
        "\n",
        "            # Get the class_id\n",
        "            class_id= detection[1]\n",
        "            # Map the class id to the class\n",
        "            class_name = CLASS_NAMES[int(class_id)-1]\n",
        "\n",
        "            # Draw bounding boxes only if the detection confidence is above a certain threshold and the detected object is\n",
        "            # a person (a.k.a, pedestrians in this assignment), else skip\n",
        "            if class_name == \"person\" and confidence > 0.4:\n",
        "                # Get the boudning box coordinates\n",
        "                x_start = detection[3] * image_width\n",
        "                y_start = detection[4] * image_height\n",
        "                x_end = detection[5] * image_width\n",
        "                y_end = detection[6] * image_height\n",
        "                # Get the boudning box width\n",
        "                w = x_end - x_start\n",
        "                # h = y_end - y_start\n",
        "\n",
        "                rects.append(np.array([x_start, y_start, x_end, y_end]))\n",
        "\n",
        "                # Get the centroid of the bounding box\n",
        "                cx = int((x_start + x_end) / 2.0)\n",
        "                cy = int((y_start + y_end) / 2.0)\n",
        "\n",
        "                centroids.append([cx, cy])\n",
        "\n",
        "                # Estimate the distance between the pedestrians and the camera\n",
        "                object_dist = estimate_distance(focal_length_found, KNOWN_WIDTH, w)\n",
        "\n",
        "                # If there are less than 3 pedestrians detected in this frame, store the estimated tracking distance and the location of\n",
        "                # that person. Else, compare the distance of the current detected pedestrians to the longest distance being tracked in the\n",
        "                # camera_obj_dist_tracking list. If the distance of the current detected pedestrians is less than the longest distance being\n",
        "                # tracked in the camera_obj_dist_tracking list, update the list.\n",
        "                if len(camera_obj_dist_tracking) < 3:\n",
        "                    camera_obj_dist_tracking.append(object_dist)\n",
        "                    box_info.append([x_start, y_start, x_end, y_end, cx, cy])\n",
        "                else:\n",
        "                    # it is possible to have more than 1 max distances in the list (distance values are the same)\n",
        "                    index_of_max_dist = [index for index, item in enumerate(camera_obj_dist_tracking) if item == max(camera_obj_dist_tracking)]\n",
        "                    if object_dist < camera_obj_dist_tracking[index_of_max_dist[0]]:\n",
        "                        camera_obj_dist_tracking[index_of_max_dist[0]] = object_dist\n",
        "                        box_info[index_of_max_dist[0]] = [x_start, y_start, x_end, y_end, cx, cy]\n",
        "\n",
        "                # Draw a bounding box for the detected pedestrian\n",
        "                cv.rectangle(frame_2, (int(x_start), int(y_start)), (int(x_end), int(y_end)), (0, 255, 0), 2)\n",
        "                cv.rectangle(frame_3, (int(x_start), int(y_start)), (int(x_end), int(y_end)), (0, 255, 0), 2)\n",
        "                cv.rectangle(frame_4, (int(x_start), int(y_start)), (int(x_end), int(y_end)), (0, 255, 0), 2)\n",
        "\n",
        "        # Update the tracking information of detected pedestrians\n",
        "        objects = ct.update(rects)\n",
        "\n",
        "        # Display the tracking information in the frame - FRAME 3\n",
        "        centroids = np.asarray(centroids)\n",
        "        # If there is no pedestrian detected in the current frame, skip. Else, add labels to the bounding boxes\n",
        "        if len(centroids) != 0:\n",
        "            for (objectID, centroid) in objects.items():\n",
        "                # If the tracked centroids match the detected centroids in this frame, add labels (In some cases, some pedestrians cannot be detected\n",
        "                # in successive frames, but there tracking information is still being stored since their number of disapperance haven't exceeded the\n",
        "                # maximum disapperance times. However, we don't want to draw that label in this current frame because that person cannot be detected\n",
        "                # in this frame)\n",
        "                if np.any(np.all(centroid == centroids, axis=1)):\n",
        "                    index = np.where(centroids == centroid)[0][0]\n",
        "                    x_start, y_start = rects[index][0], rects[index][1]\n",
        "                    text = \"ID: {}\".format(objectID)\n",
        "                    cv.putText(frame_3, text, (int(x_start), int(y_start - 5)), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "        # Display up to 3 closest pedestrians to the camera - FRAME 4\n",
        "        dist_order = 1\n",
        "        for i in range (0, len(camera_obj_dist_tracking)):\n",
        "            index_of_min_dist = [index for index, item in enumerate(camera_obj_dist_tracking) if item == min(camera_obj_dist_tracking)]\n",
        "            x_start, y_start, x_end, y_end, cx, cy = box_info[index_of_min_dist[0]]\n",
        "            # Draw a red box for up to 3 closest pedestrians to the camera. Other detected pedestrians are still being drawn in green\n",
        "            cv.rectangle(frame_4, (int(x_start), int(y_start)), (int(x_end), int(y_end)), (0, 0, 255), 2)\n",
        "\n",
        "            # Add the order closeness (1 - closest to the camera, 2 - 2nd closest to the camera, 3rd - 3rd closest to the camera)\n",
        "            text = \"Ord: {}\".format(dist_order)\n",
        "            cv.putText(frame_4, text, (int(x_start), int(y_start - 5)), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "\n",
        "            camera_obj_dist_tracking.pop(index_of_min_dist[0])\n",
        "            box_info.pop(index_of_min_dist[0])\n",
        "\n",
        "            dist_order += 1\n",
        "\n",
        "        # Concatenate to display\n",
        "        first_row = cv.hconcat([frame_1, frame_2])\n",
        "        second_row = cv.hconcat([frame_3, frame_4])\n",
        "        stack = cv.vconcat((first_row, second_row))\n",
        "        cv.imshow(\"Task 2's Output\", stack)\n",
        "\n",
        "        key = cv.waitKey(20)\n",
        "        if key == 27:\n",
        "            break"
      ],
      "metadata": {
        "id": "zOk-_9Rezmg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    video_name = \"...\" # Fill in the video name\n",
        "    person_tracking(video_name)"
      ],
      "metadata": {
        "id": "t7FNX6DdzrRq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}